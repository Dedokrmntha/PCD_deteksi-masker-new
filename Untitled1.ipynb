{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dbc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94f85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFaceBox(net, frame,conf_threshold = 0.75):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn,1.0,(300,300),\n",
    "                                 [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0,0,i,2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0,0,i,3]* frameWidth)\n",
    "            y1 = int(detections[0,0,i,4]* frameHeight)\n",
    "            x2 = int(detections[0,0,i,5]* frameWidth)\n",
    "            y2 = int(detections[0,0,i,6]* frameHeight)\n",
    "            bboxes.append([x1,y1,x2,y2])\n",
    "            cv2.rectangle(frameOpencvDnn,(x1,y1),(x2,y2),(0,255,0),\n",
    "                          int(round(frameHeight/150)),8)\n",
    "\n",
    "    return frameOpencvDnn , bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576ed806",
   "metadata": {},
   "outputs": [],
   "source": [
    "aceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d55d33b",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"age_deploy.prototxt\" in function 'cv::dnn::ReadProtoFromTextFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#load the network\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ageNet \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mageModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mageProto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m genderNet \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(genderModel, genderProto)\n\u001b[0;32m      4\u001b[0m faceNet \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(faceModel, faceProto)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"age_deploy.prototxt\" in function 'cv::dnn::ReadProtoFromTextFile'\n"
     ]
    }
   ],
   "source": [
    "#load the network\n",
    "ageNet = cv2.dnn.readNet(ageModel,ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "padding = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b218e97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faceNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#creating a smaller frame for better optimization\u001b[39;00m\n\u001b[0;32m     10\u001b[0m small_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame,(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m),fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,fy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m frameFace ,bboxes \u001b[38;5;241m=\u001b[39m getFaceBox(\u001b[43mfaceNet\u001b[49m,small_frame)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bboxes:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo face Detected, Checking next frame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'faceNet' is not defined"
     ]
    }
   ],
   "source": [
    "while cv2.waitKey(1) < 0:\n",
    "    #read frame\n",
    "    t = time.time()\n",
    "    hasFrame , frame = cap.read()\n",
    "\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "    #creating a smaller frame for better optimization\n",
    "    small_frame = cv2.resize(frame,(0,0),fx = 0.5,fy = 0.5)\n",
    "\n",
    "    frameFace ,bboxes = getFaceBox(faceNet,small_frame)\n",
    "    if not bboxes:\n",
    "        print(\"No face Detected, Checking next frame\")\n",
    "        continue\n",
    "    for bbox in bboxes:\n",
    "        face = small_frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),\n",
    "                max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "        blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        print(\"Gender : {}, conf = {:.3f}\".format(gender, genderPreds[0].max()))\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds = ageNet.forward()\n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        print(\"Age Output : {}\".format(agePreds))\n",
    "        print(\"Age : {}, conf = {:.3f}\".format(age, agePreds[0].max()))\n",
    "\n",
    "        label = \"{},{}\".format(gender, age)\n",
    "        cv2.putText(frameFace, label, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Age Gender Demo\", frameFace)\n",
    "       \n",
    "    print(\"time : {:.3f}\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b3799dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimutils\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cv2\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetectshape\u001b[39m(c):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# initialize the shape name and approximate the contour\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imutils'"
     ]
    }
   ],
   "source": [
    "import imutils\n",
    "from cv2 import cv2\n",
    "\n",
    "def detectshape(c):\n",
    "# initialize the shape name and approximate the contour\n",
    "        shape = \"unidentified\"\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.04 * peri, True)\n",
    "\n",
    "\t# if the shape is a triangle, it will have 3 vertices\n",
    "        if len(approx) == 3:\n",
    "                shape = \"triangle\"\n",
    "\n",
    "\t# if the shape has 4 vertices, it is either a square or\n",
    "\t# a rectangle\n",
    "        elif len(approx) == 4:\n",
    "                # compute the bounding box of the contour and use the\n",
    "\t\t# bounding box to compute the aspect ratio\n",
    "                (x, y, w, h) = cv2.boundingRect(approx)\n",
    "                ar = w / float(h)\n",
    "\n",
    "\t\t# a square will have an aspect ratio that is approximately\n",
    "\t\t# equal to one, otherwise, the shape is a rectangle\n",
    "                shape = \"square\" if ar >= 0.95 and ar <= 1.05 else \"rectangle\"\n",
    "\n",
    "\t# if the shape is a pentagon, it will have 5 vertices\n",
    "        elif len(approx) == 5:\n",
    "                shape = \"pentagon\"\n",
    "\n",
    "\t# otherwise, we assume the shape is a circle\n",
    "        else:\n",
    "                shape = \"circle\"\n",
    "\n",
    "\t# return the name of the shape\n",
    "        return shape\n",
    "\n",
    "image = cv2.imread(\"shapeUji8.png\")\n",
    "resized = imutils.resize(image, width=300)\n",
    "ratio = image.shape[0] / float(resized.shape[0])\n",
    "\n",
    "gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "thresh = cv2.threshold(blurred, 60, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "# find contours and hierarchy in the  image and initialize the\n",
    "# shape detector\n",
    "cnts,hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#list \n",
    "shapes = []\n",
    "shapesSgtg = []\n",
    "results = []\n",
    "\n",
    "# detecting shape square and triangle, and save the index number\n",
    "for cIdx, c in enumerate(cnts):\n",
    "\tshape = detectshape(c)\n",
    "\tif shape == \"square\":\n",
    "\t\tshapes.append(cIdx)\n",
    "\tif shape == \"triangle\":\n",
    "\t\tshapesSgtg.append(cIdx)\n",
    "\n",
    "# checking, is the triangle a child of square\n",
    "for x in shapesSgtg:\n",
    "\tfor y in shapes:\n",
    "\t\tif hierarchy[0][x][3] == y:\n",
    "\t\t\tresults.append(x)\n",
    "\n",
    "# marking the detection in the image and labelling\n",
    "for result in results:\n",
    "\tcounts = cnts[result]\n",
    "\tM = cv2.moments(counts)\n",
    "\tcX = int((M[\"m10\"] / M[\"m00\"]) * ratio)\n",
    "\tcY = int((M[\"m01\"] / M[\"m00\"]) * ratio)\n",
    "\tcounts = counts.astype(\"float\")\n",
    "\tcounts *= ratio\n",
    "\tcounts = counts.astype(\"int\")\n",
    "\tcv2.drawContours(image, [counts], -1, (0, 255, 0), 2)\n",
    "\tcv2.putText(image, \"Segitiga dalam Kotak\", (cX, cY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "#show the result\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c1a11e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RMSprop\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "     \n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "     \n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "TRAINING_DIR = \"data training\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=20, \n",
    "                                                    target_size=(150, 150))\n",
    "VALIDATION_DIR = \"data validation\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=20, \n",
    "                                                         target_size=(150, 150))\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                              epochs=5,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    "plt.plot(history.history['loss'],'r',label='training loss')\n",
    "plt.plot(history.history['val_loss'],label='validation loss')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    " \n",
    "plt.plot(history.history['acc'],'r',label='training accuracy')\n",
    "plt.plot(history.history['val_acc'],label='validation accuracy')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c58764a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m=\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel2-004.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m results\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTanpa masker\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMenggunakan masker\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "model=load_model(\"model2-004.model\")\n",
    " \n",
    "results={0:'Tanpa masker',1:'Menggunakan masker'}\n",
    "GR_dict={0:(0,0,255),1:(0,255,0)}\n",
    " \n",
    "rect_size = 4\n",
    "cap = cv2.VideoCapture(0) \n",
    " \n",
    " \n",
    "haarcascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    " \n",
    "while True:\n",
    "    (rval, im) = cap.read()\n",
    "    im=cv2.flip(im,1,1) \n",
    " \n",
    "     \n",
    "    rerect_size = cv2.resize(im, (im.shape[1] // rect_size, im.shape[0] // rect_size))\n",
    "    faces = haarcascade.detectMultiScale(rerect_size)\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * rect_size for v in f] \n",
    "         \n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        rerect_sized=cv2.resize(face_img,(150,150))\n",
    "        normalized=rerect_sized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    " \n",
    "         \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "       \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),GR_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),GR_dict[label],-1)\n",
    "        cv2.putText(im, results[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    " \n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "     \n",
    "    if key == 27: \n",
    "        break\n",
    " \n",
    "    cap.release()\n",
    " \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03fb553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
